import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# Carregar o dataset (depois de baixar e enviar para o Colab ou usar um link direto)
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt"
column_names = ['area', 'perimeter', 'compactness', 'length_kernel', 'width_kernel',
                'asymmetry_coeff', 'length_groove', 'variety']
df = pd.read_csv(url, sep="\s+", names=column_names)

df.head()

df.describe()

# Histogramas
df.hist(bins=15, figsize=(12, 8))
plt.tight_layout()
plt.show()

# Boxplots
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.iloc[:, :-1])
plt.xticks(rotation=45)
plt.title("Distribuição dos atributos")
plt.show()

sns.pairplot(df, hue='variety', palette='husl')
plt.show()

scaler = StandardScaler()  
X = scaler.fit_transform(df.drop('variety', axis=1))
y = df['variety']

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
import seaborn as sns
import matplotlib.pyplot as plt

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

models = {
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "SVM": SVC(kernel='rbf', C=1.0),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Naive Bayes": GaussianNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000)
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    acc = accuracy_score(y_test, y_pred)
    results[name] = {
        "accuracy": acc,
        "report": classification_report(y_test, y_pred, output_dict=True),
        "confusion_matrix": confusion_matrix(y_test, y_pred)
    }
    print(f"Modelo: {name}")
    print(f"Acurácia: {acc:.4f}")
    print("Relatório de Classificação:")
    print(classification_report(y_test, y_pred))
    print("="*60)
for name in results:
    cm = results[name]["confusion_matrix"]
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f"Matriz de Confusão - {name}")
    plt.xlabel("Previsto")
    plt.ylabel("Real")
    plt.show()
from sklearn.model_selection import GridSearchCV
param_grid_knn = {'n_neighbors': range(1, 21)}

grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)
grid_knn.fit(X_train, y_train)

print(f"Melhor parâmetro para KNN: {grid_knn.best_params_}")
print(f"Melhor acurácia no treino: {grid_knn.best_score_:.4f}")

param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)
grid_svm.fit(X_train, y_train)

print(f"Melhor parâmetro para SVM: {grid_svm.best_params_}")
print(f"Melhor acurácia no treino: {grid_svm.best_score_:.4f}")

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5)
grid_rf.fit(X_train, y_train)

print(f"Melhor parâmetro para Random Forest: {grid_rf.best_params_}")
print(f"Melhor acurácia no treino: {grid_rf.best_score_:.4f}")

best_models = {
    "KNN Otimizado": grid_knn.best_estimator_,
    "SVM Otimizado": grid_svm.best_estimator_,
    "Random Forest Otimizado": grid_rf.best_estimator_
}

for name, model in best_models.items():
    y_pred = model.predict(X_test)
    print(f"\n{name}")
    print(f"Acurácia: {accuracy_score(y_test, y_pred):.4f}")
    print("Relatório de Classificação:")
    print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt


model_names = ['KNN', 'SVM', 'Random Forest']
acc_before = [0.83, 0.82, 0.92]  
acc_after = [0.90, 0.88, 0.88]   

x = range(len(model_names))
plt.figure(figsize=(8, 5))
plt.bar(x, acc_before, width=0.4, label='Antes', align='center')
plt.bar([i + 0.4 for i in x], acc_after, width=0.4, label='Depois', align='center')
plt.xticks([i + 0.2 for i in x], model_names)
plt.ylabel('Acurácia')
plt.title('Comparação de Acurácia dos Modelos (Antes x Depois da Otimização)')
plt.legend()
plt.ylim(0.8, 1.0)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

